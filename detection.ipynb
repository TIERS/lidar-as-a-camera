{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af021c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import detection\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "import torchvision\n",
    "from PIL import *\n",
    "import random\n",
    "import skimage\n",
    "from skimage.io import imread, imshow\n",
    "import re\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# from iou import intersection_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c83d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'bike'\n",
    "w = 1000\n",
    "h = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f385cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15dd261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = detection.(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8611b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = os.listdir(folder_path)\n",
    "# annot_list = os.listdir(annot_path)\n",
    "data =[]\n",
    "pred = []\n",
    "gt = []\n",
    "gen_list=[]\n",
    "for i in range(len(data_list)):\n",
    "    img_path = os.path.join(folder_path, data_list[i])\n",
    "#     anot_path = os.path.join(annot_path, annot_list[i])\n",
    "    image = imread(img_path) \n",
    "    dim = (w, h)\n",
    "    image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "#     image = cv2.boxFilter(image, -1, (3, 3))\n",
    "    image1 = image.transpose((2, 0, 1))    \n",
    "    image1 = np.expand_dims(image1, axis=0)\n",
    "    image1 = image1 / 255.0\n",
    "    image1 = torch.FloatTensor(image1)\n",
    "\n",
    "    \n",
    "    predictions = model(image1)\n",
    "    boxes = predictions[0]['boxes'].detach()\n",
    "    boxes = boxes.cpu().numpy().astype(np.uint16).tolist()\n",
    "    scores = predictions[0]['scores'].detach().cpu().numpy().tolist()\n",
    "    labels = predictions[0]['labels'].detach().cpu().numpy().tolist()\n",
    "    for label, box,s in zip(labels, boxes, scores):\n",
    "        if s<0.4:\n",
    "            continue\n",
    "        pred.append([i,s,[COCO_INSTANCE_CATEGORY_NAMES[label],box]])\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[2], box[3] ), color=(0,255,0), thickness=2)\n",
    "        pred_class = COCO_INSTANCE_CATEGORY_NAMES[label]\n",
    "        conf = \"{} %\".format(round(s,2))\n",
    "        cv2.putText(image,pred_class, (box[0]-10, box[1]-20), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,255,0),thickness=1)\n",
    "        cv2.putText(image,conf, (box[0]-10, box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,255,0),thickness=1)\n",
    "    cv2.imwrite('mask_bike/bike{}.png'.format(i),image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54fdfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b90282",
   "metadata": {},
   "outputs": [],
   "source": [
    "person=0\n",
    "car=0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i][2][0]=='person':\n",
    "        person +=1\n",
    "    if pred[i][2][0]=='car':\n",
    "        car +=1\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fe4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ffdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = []\n",
    "ground_truths = []\n",
    "\n",
    "# Go through all predictions and targets,\n",
    "# and only add the ones that belong to the\n",
    "# current class c\n",
    "for detection in pred:\n",
    "    if detection[2][0] == 1:\n",
    "        detections.append(detection)\n",
    "\n",
    "for true_box in gt:\n",
    "    for jj in range(len(true_box[1])):\n",
    "        if true_box[1][jj][0] == 1:\n",
    "            ground_truths.append([true_box[0],[1, true_box[1][jj][1:]]])\n",
    "\n",
    "\n",
    "detections.sort(key=lambda x: x[1], reverse=True)\n",
    "TP = torch.zeros((len(detections)))\n",
    "FP = torch.zeros((len(detections)))\n",
    "\n",
    "\n",
    "for detection_idx, detection in enumerate(detections):\n",
    "    print(detection[0])\n",
    "    print(detection[2][1])\n",
    "    \n",
    "    # Only take out the ground_truths that have the same\n",
    "    # training idx as detection\n",
    "    ground_truth_img = [\n",
    "        bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "    ]\n",
    "\n",
    "    num_gts = len(ground_truth_img)\n",
    "    best_iou = 0\n",
    "\n",
    "    for idx, gtr in enumerate(ground_truth_img):\n",
    "        print(gtr[1][1])\n",
    "#         iou = intersection_over_union(\n",
    "#             torch.tensor(detection[2][1]),\n",
    "#             torch.tensor(gtr[3:]),\n",
    "#             box_format=\"midpoint\",\n",
    "# #         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for true_box in gt:\n",
    "    for jj in range(len(true_box[1])):\n",
    "        if true_box[1][jj][0] == c:\n",
    "            ground_truths.append([true_box[0],[c, true_box[1][jj][1:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a829d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list storing all AP for respective classes\n",
    "average_precisions = []\n",
    "\n",
    "# used for numerical stability later on\n",
    "epsilon = 1e-6\n",
    "\n",
    "for c in range(5):\n",
    "    detections = []\n",
    "    ground_truths = []\n",
    "\n",
    "    # Go through all predictions and targets,\n",
    "    # and only add the ones that belong to the\n",
    "    # current class c\n",
    "    for detection in pred:\n",
    "        if detection[2][0] == c:\n",
    "            detections.append(detection)\n",
    "\n",
    "    for true_box in gt:\n",
    "        for jj in range(len(true_box[1])):\n",
    "            if true_box[1][jj][0] == c:\n",
    "                ground_truths.append([true_box[0],[c, true_box[1][jj][1:]]])\n",
    "\n",
    "    # find the amount of bboxes for each training example\n",
    "    # Counter here finds how many ground truth bboxes we get\n",
    "    # for each training example, so let's say img 0 has 3,\n",
    "    # img 1 has 5 then we will obtain a dictionary with:\n",
    "    # amount_bboxes = {0:3, 1:5}\n",
    "    amount_bboxes = Counter([gtr[0] for gtr in ground_truths])\n",
    "\n",
    "    # We then go through each key, val in this dictionary\n",
    "    # and convert to the following (w.r.t same example):\n",
    "    # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "    for key, val in amount_bboxes.items():\n",
    "        amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "    # sort by box probabilities which is index 2\n",
    "    detections.sort(key=lambda x: x[1], reverse=True)\n",
    "    TP = torch.zeros((len(detections)))\n",
    "    FP = torch.zeros((len(detections)))\n",
    "    total_true_bboxes = len(ground_truths)\n",
    "\n",
    "    # If none exists for this class then we can safely skip\n",
    "    if total_true_bboxes == 0:\n",
    "        continue\n",
    "\n",
    "    for detection_idx, detection in enumerate(detections):\n",
    "        # Only take out the ground_truths that have the same\n",
    "        # training idx as detection\n",
    "        ground_truth_img = [\n",
    "            bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "        ]\n",
    "\n",
    "        num_gts = len(ground_truth_img)\n",
    "        best_iou = 0\n",
    "\n",
    "        for idx, gtr in enumerate(ground_truth_img):\n",
    "            iou = intersection_over_union(\n",
    "                torch.tensor(detection[2][1]),\n",
    "                torch.tensor(gtr[1][1]),\n",
    "                box_format=\"midpoint\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8896447",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a487819",
   "metadata": {},
   "outputs": [],
   "source": [
    "for detection_idx, detection in enumerate(detections):\n",
    "    print(detection_idx,detection[0])\n",
    "#     ground_truth_img = [\n",
    "#             bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c19e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections.sort(key=lambda x: x[1], reverse=True)\n",
    "TP = torch.zeros((len(detections)))\n",
    "FP = torch.zeros((len(detections)))\n",
    "total_true_bboxes = len(ground_truths)\n",
    "for detection_idx, detection in enumerate(detections):\n",
    "    # Only take out the ground_truths that have the same\n",
    "    # training idx as detection\n",
    "    ground_truth_img = [\n",
    "        bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "    ]\n",
    "\n",
    "    num_gts = len(ground_truth_img)\n",
    "    best_iou = 0\n",
    "\n",
    "    for idx, gt in enumerate(ground_truth_img):\n",
    "        iou = intersection_over_union(\n",
    "            torch.tensor(detection[3:]),\n",
    "            torch.tensor(gt[3:]),\n",
    "            box_format=\"midpoint\",\n",
    "        )\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_gt_idx = idx\n",
    "\n",
    "    if best_iou > 0.5:\n",
    "        # only detect ground truth detection once\n",
    "        if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "            # true positive and add this bounding box to seen\n",
    "            TP[detection_idx] = 1\n",
    "            amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "        else:\n",
    "            FP[detection_idx] = 1\n",
    "\n",
    "    # if IOU is lower then the detection is a false positive\n",
    "    else:\n",
    "        FP[detection_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths =[]\n",
    "# for c in range(12):\n",
    "c=0\n",
    "for true_box in gt:\n",
    "    for jj in range(len(true_box[1])):\n",
    "        if true_box[1][jj][0] == 2:\n",
    "            ground_truths.append([true_box[0],[c, true_box[1][jj][1:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42545e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_bboxes = Counter([gtr[0] for gtr in ground_truths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ebfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in amount_bboxes.items():\n",
    "    amount_bboxes[key] = torch.zeros(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"corners\", num_classes=4\n",
    "):\n",
    "\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[2][0] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            for jj in range(len(true_box[1])):\n",
    "                if true_box[1][jj][0] == c:\n",
    "                    ground_truths.append([true_box[0],[c, true_box[1][jj][1:]]])\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gtr[0] for gtr in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[1], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "        \n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gtr in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[2][1]),\n",
    "                    torch.tensor(gtr[1][1]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou =mean_average_precision(pred, gt, iou_threshold=0.5, box_format=\"midpoint\", num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b211bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection.ipynb  \u001b[0m\u001b[01;34mmask\u001b[0m/  \u001b[01;34mresized\u001b[0m/  \u001b[01;34mselected\u001b[0m/  yolov5s.pt  \u001b[01;34myolox_m\u001b[0m/\r\n",
      "\u001b[01;34mfaster\u001b[0m/          \u001b[01;34mnew\u001b[0m/   \u001b[01;34mretina\u001b[0m/   \u001b[01;34myolov5\u001b[0m/    \u001b[01;34myolox\u001b[0m/      \u001b[01;34myolox_s\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa0d94",
   "metadata": {},
   "source": [
    "## YOLOV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e20d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sahar/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-2-15 torch 1.8.0+cu111 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
      "Adding AutoShape... \n",
      "Saved 1 image to \u001b[1mruns/detect/exp\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp2\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp3\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp4\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp6\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp7\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp8\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp9\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp10\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp11\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp12\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp13\u001b[0m\n",
      "Saved 1 image to \u001b[1mruns/detect/exp14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'bike'\n",
    "w = 1000\n",
    "h = 300\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.conf=0.33\n",
    "data_list = os.listdir(folder_path)\n",
    "# annot_list = os.listdir(annot_path)\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    img_path = os.path.join(folder_path, data_list[i])\n",
    "#     anot_path = os.path.join(annot_path, annot_list[i])\n",
    "    image = imread(img_path) \n",
    "    dim = (w, h)\n",
    "    image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    results = model(image)\n",
    "    results.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0083ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection.ipynb  \u001b[0m\u001b[01;34mmask_seg\u001b[0m/   \u001b[01;34mresized\u001b[0m/  \u001b[01;34mselected\u001b[0m/   \u001b[01;34myolox\u001b[0m/\r\n",
      "\u001b[01;34mfaster\u001b[0m/          \u001b[01;34mnew\u001b[0m/        \u001b[01;34mretina\u001b[0m/   \u001b[01;34myolov5\u001b[0m/     \u001b[01;34myolox_m\u001b[0m/\r\n",
      "\u001b[01;34mmask\u001b[0m/            \u001b[01;34mpointrend\u001b[0m/  \u001b[01;34mruns\u001b[0m/     yolov5s.pt  \u001b[01;34myolox_s\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617dc288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sahar/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-2-15 torch 1.8.0+cu111 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
      "Adding AutoShape... \n",
      "Saved 1 image to \u001b[1mruns/detect/exp60\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.conf = 0.4\n",
    "image = imread('resized/pre41.png') \n",
    "# dim = (w, h)\n",
    "# image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "results = model(image)\n",
    "results.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517b563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
